# 1.异常检测

**异常检测(Outlier Detection)** ：识别不正常（与预期行为差异大）的数据；针对少数、不可预测或不确定、罕见的事件时，它具有独特的复杂性，使得一般的机器学习和深度学习技术无效。

阅读论文时遇到的名词皆为异常检测：Outlier Detection，Anomaly Detection，Novelty Detection，Forgery Detection，Out-of-distribution Detection

Eg.信用卡欺诈，工业生产异常，网络流里的异常（网络侵入）等问题，针对的是少数的事件



## 1.1 异常的种类

-   **点异常**（point anomalies）指的是少数个体实例是异常的，大多数个体实例是正常的，例如正常人与病人的健康指标；

下图中$o_{1}, o_{2}$两点属于点异常

![img](https://img-blog.csdnimg.cn/20210409213448662.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaXl1enV4aWFxaWFubGk=,size_16,color_FFFFFF,t_70#pic_center)



-   **条件异常**（conditional anomalies），又称**上下文异常**，指的是在特定情境下个体实例是异常的，在其他情境下都是正常的，例如在特定时间下的温度突然上升或下降，在特定场景中的快速信用卡交易；

下图中$t_{2}$属于上下文异常

![img](https://img-blog.csdnimg.cn/20210409213736935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaXl1enV4aWFxaWFubGk=,size_16,color_FFFFFF,t_70#pic_center)



-   **群体异常**（group anomalies）指的是在群体集合中的个体实例出现异常的情况，而该个体实例自身可能不是异常，在入侵或欺诈检测等应用中，离群点对应于多个数据点的序列，而不是单个数据点。例如社交网络中虚假账号形成的集合作为群体异常子集，但子集中的个体节点可能与真实账号一样正常。

下图中红框区域属于群体异常![img](https://img-blog.csdnimg.cn/20210409214011612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaXl1enV4aWFxaWFubGk=,size_16,color_FFFFFF,t_70#pic_center)



## 1.2 异常检测数据集分类

-   统计型数据static data（文本、网络流）
    
-   序列型数据sequential data（sensor data ）
    
- 空间型数据spatial data（图像、视频）

  

## 1.3 异常检测任务分类

-   **有监督**：训练集的正例和反例均有标签—在训练集中的正常实例和异常实例都有标签，这类方法的缺点在于数据标签难以获得或数据不均衡（正常样本数量远大于异常样本数量）
    
-   **无监督**：训练集无标签—在训练集中既有正常实例也可能存在异常实例，但假设数据的比例是正常实例远大于异常实例，模型训练过程中没有标签进行校正。
    
- **半监督**：在训练集中只有正例，异常实例不参与训练—目前很多异常检测研究都集中在半监督方法上，有很多声称是无监督异常检测方法的研究其实也是半监督的，对其解释的是该异常检测是无监督异常检测，学习特征的方式是无监督的，但是评价方式使用了半监督的方法，因此对于无监督与半监督的界定感觉没有那么规范。

  

## 1.4 异常检测场景

-   **故障检测**：主要是监控系统，在故障发生时可以识别，并且准确指出故障的种类以及出现位置。主要应用领域包括银行欺诈、移动蜂窝网络故障、保险欺诈、医疗欺诈。
    
-   **医疗日常检测**：在许多医疗应用中，数据是从各种设备收集的，如磁共振成像（MRI）扫描、正电子发射断层扫描（PET）扫描或心电图（ECG）时间序列。这些数据中的异常模式通常反映疾病状况。
    
-   **入侵检测**（Intrusion detection）：通过从计算机网络或计算机系统中的若干关键点收集信息并对其执行分析，从中发觉网络或系统中能不能有违反安全策略的行为和遭到袭击的迹象（有关操作系统调用、网络流量或其他用户操作的不同类型的数据，由于恶意活动，此数据可能显示异常行为），并对此做出适当反应的流程。
    
    最普遍的两种入侵检测系统包括基于主机的入侵检测系统（HIDS）、网络入侵检测系统（NIDS）。
    
-   **欺诈检测**：信用卡欺诈越来越普遍，因为信用卡号码等敏感信息更容易被泄露。在许多情况下，未经授权使用信用卡可能表现出不同的模式，例如从特定地点疯狂购买或进行非常大的交易。这种模式可用于检测信用卡交易数据中的异常值。
    
-   **工业异常检测**（Industrial Anomalies Detection）
    
-   **时间序列异常检测**（Anomaly Detection in TimeSeries）
    
-   **视频异常检测**（Video Surveillance）：检测视频中的异常场景。
    
- **日志异常检测**（Log Anomaly Detection）

  

## 1.5 异常检测的难点

-   数据量少。异常检测任务通常情况下负样本（异常样本）是比较少的，有时候依赖于人工标签，属于样本不平衡问题。
    
-   噪音。异常和噪音有时候很难分清，如下图，图a的A点位于数据的稀疏区域，与其他数据非常不同，因此可以断定为异常，但是像图b的A点，周围有也有很多点分布，我们很难把A点识别出来。

![image-20210505101703601.png](https://github.com/datawhalechina/team-learning-data-mining/blob/master/AnomalyDetection/img/image-20210505101703601.png?raw=true)



## 参考资料

[(30条消息) 异常检测中的三种异常：点异常、上下文异常、集合异常\_千行百行的博客-CSDN博客](https://blog.csdn.net/shiyuzuxiaqianli/article/details/115560027)

[中科院在读美女博士带你全面了解“异常检测”领域 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/260651151)

[异常检测 | Anomaly Detection综述 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/266513299)



# 2.异常检测基础方法

## 2.1 基于统计学

### 2.1.1 概述

统计学方法对数据的正常性做出假定。**它们假定正常的数据对象由一个统计模型产生，而不遵守该模型的数据是异常点。**统计学方法的有效性高度依赖于对给定数据所做的统计模型假定是否成立。

* 一般思想：学习一个拟合给定数据集的生成模型，然后识别该模型低概率区域中的对象，把它们作为异常点
* 具体方法：先基于统计学方法确定一个概率分布模型，然后判断各个离散点有多大概率符合该模型

* 难点：如何得到概率分布模型
  * 首先是识别数据集的具体分布：数据的真实分布是否是现在手里的数据集完全体现的。尽管许多类型的数据都可以用常见的分布（高斯分布、泊松分布或者二项式分布）来描述，但是具有非标准分布的数据集非常常见。如果选择了错误的模型，则对象可能会被错误地识别为异常点。
  * 其次，如何确定使用属性的个数，基于统计学的方法，数据的属性一般具有一个或多个，那么在建立概率分布模型的过程中究竟是用一个属性还是多个属性需要分析和尝试。
  * 最后，当使用数据属性很多时，模型比较复杂并且难以理解，会涉及到EM算法。

* 方法：参数方法和非参数法

假定输入数据集为$\{x^{(1)}, x^{(2)}, ..., x^{(m)}\}$，数据集中的样本服从正态分布，即$x^{(i)}\sim N(\mu, \sigma^2)$，我们可以根据样本求出参数$\mu$和$\sigma$。

$\mu=\frac 1m\sum_{i=1}^m x^{(i)}$

$\sigma^2=\frac 1m\sum_{i=1}^m (x^{(i)}-\mu)^2$



## 参考资料

[异常检测统计学方法 - 简书 (jianshu.com)](https://www.jianshu.com/p/a7a9f89c6d06)



## 2.2 线性模型

### 2.2.1 概述

典型的如PCA方法，Principle Component Analysis是主成分分析，简称PCA。它的应用场景是对数据集进行降维。降维后的数据能够最大程度地保留原始数据的特征（以数据协方差为衡量标准）。其原理是通过构造一个新的特征空间，把原数据映射到这个新的低维空间里。PCA可以提高数据的计算性能，并且缓解"高维灾难"。





## 2.3 基于领近度

### 2.3.1 概述

这类算法适用于数据点的聚集程度高、离群点较少的情况。同时，因为相似度算法通常需要对每一个数据分别进行相应计算，所以这类算法通常计算量大，不太适用于数据量大、维度高的数据。    

​		基于相似度的检测方法大致可以分为三类： 

* 基于集群（簇）的检测，如DBSCAN等聚类算法。    
  	聚类算法是将数据点划分为一个个相对密集的“簇”，而那些不能被归为某个簇的点，则被视作离群点。这类算法对簇个数的选择高度敏感，数量选择不当可能造成较多正常值被划为离群点或成小簇的离群点被归为正常。因此对于每一个数据集需要设置特定的参数，才可以保证聚类的效果，在数据集之间的通用性较差。聚类的主要目的通常是为了寻找成簇的数据，而将异常值和噪声一同作为无价值的数据而忽略或丢弃，在专门的异常点检测中使用较少。    

+ 基于距离的度量，如k近邻算法。    

  ​		k近邻算法的基本思路是对每一个点，计算其与最近k个相邻点的距离，通过距离的大小来判断它是否为离群点。在这里，离群距离大小对k的取值高度敏感。如果k太小（例如1），则少量的邻近离群点可能导致较低的离群点得分；如果k太大，则点数少于k的簇中所有的对象可能都成了离群点。为了使模型更加稳定，距离值的计算通常使用k个最近邻的平均距离。 

+ 基于密度的度量，如LOF（局部离群因子）算法。 

  ​		局部离群因子（LOF）算法与k近邻类似，不同的是它以相对于其邻居的局部密度偏差而不是距离来进行度量。它将相邻点之间的距离进一步转化为“邻域”，从而得到邻域中点的数量（即密度），认为密度远低于其邻居的样本为异常值。   



# 3 集成方法

集成是提高数据挖掘算法精度的常用方法。集成方法将多个算法或多个基检测器的输出结合起来。其基本思想是一些算法在某些子集上表现很好，一些算法在其他子集上表现很好，然后集成起来使得输出更加鲁棒。集成方法与基于子空间方法有着天然的相似性，子空间与不同的点集相关，而集成方法使用基检测器来探索不同维度的子集，将这些基学习器集合起来。

常用的集成方法有Feature bagging，孤立森林等。

## 3.2 Feature bagging

### 3.2.1 概述

与bagging法类似，只是对象是feature。





## 3.3 孤立森林

### 3.3.1 概述

孤立森林假设我们用一个随机超平面来切割数据空间，切一次可以生成两个子空间。然后我们继续用随机超平面来切割每个子空间并循环，直到每个子空间只有一个数据点为止。直观上来讲，那些具有高密度的簇需要被切很多次才会将其分离，而那些低密度的点很快就被单独分配到一个子空间了。孤立森林认为这些很快被孤立的点就是异常点。

用四个样本做简单直观的理解，d是最早被孤立出来的，所以d最有可能是异常。



![img](https://pic3.zhimg.com/80/v2-bb94bcf07ced88315d0a5de47677200e_720w.png)

### 

## 3.4 机器学习

### 3.4.1 概述

在有标签的情况下，可以使用树模型（gbdt,xgboost等）进行分类，缺点是异常检测场景下数据标签是不均衡的，但是利用机器学习算法的好处是可以构造不同特征。







